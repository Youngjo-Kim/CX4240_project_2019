{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_json('train.json')  ### Load data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ingredients=[] ### Initialize the list for all ingredients \n",
    "for i in range(len(data.id)):\n",
    "    for k in data.ingredients[i]:\n",
    "        if k not in unique_ingredients:\n",
    "            unique_ingredients.append(k) ## Append the ingredient if it is not in the unique_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(unique_ingredients)\n",
    "M = len(data.id)\n",
    "binary_assignment=[] ## Initialize the binary_assignment as a list\n",
    "for i in range(M):\n",
    "    assigment=np.zeros(N) ### initialze the ingredients as all 0\n",
    "    for j in data.ingredients[i]:\n",
    "        assigment[unique_ingredients.index(j)]=1 ### Assigne 1 if the ingreident is in the dish\n",
    "    binary_assignment.append(assigment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_assignment=np.reshape(binary_assignment,(M,N)) ## Reshape the assignemt into M by N matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "unique_labels=np.unique(data.cuisine)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data.cuisine)\n",
    "label=le.transform(data.cuisine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(binary_assignment, label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "America=['cajun_creole','southern_us','jamaican','mexican','brazilian']\n",
    "Asia=['chinese','filipino','thai','vietnamese','japanese','korean','indian']\n",
    "EU=['french','greek','italian','russian','spanish','irish','british']\n",
    "Africa=['moroccan']\n",
    "labels2=[]\n",
    "for i in data.cuisine:\n",
    "   if i in America:\n",
    "       labels2.append(0)\n",
    "   elif i in Asia:\n",
    "       labels2.append(1)\n",
    "   elif i in EU:\n",
    "       labels2.append(2)\n",
    "   else:\n",
    "       labels2.append(3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use <br>\n",
    "America_dataframe_with_ingredients_final, <br>\n",
    "Asia_dataframe_with_ingredients_final, <br>\n",
    "EU_dataframe_with_ingredients_final, <br>\n",
    "Other_dataframe_with_ingredients_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_dataframe = pd.DataFrame(labels2, columns = ['labels_by_continent'])\n",
    "data_conct = pd.concat([data, Label_dataframe], axis=1)\n",
    "data_conct\n",
    "\n",
    "America_dataframe = data_conct.loc[data_conct['labels_by_continent'] == 0]\n",
    "Asia_dataframe = data_conct.loc[data_conct['labels_by_continent'] == 1]\n",
    "EU_dataframe = data_conct.loc[data_conct['labels_by_continent'] == 2]\n",
    "Other_dataframe = data_conct.loc[data_conct['labels_by_continent'] == 3]\n",
    "\n",
    "America_dataframe_shape = np.array(America_dataframe.index).shape[0]    \n",
    "Asia_dataframe_shape = np.array(Asia_dataframe.index).shape[0] \n",
    "EU_dataframe_shape = np.array(EU_dataframe.index).shape[0] \n",
    "Other_dataframe_shape = np.array(Other_dataframe.index).shape[0] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############America\n",
    "# We reduce the features from 6714 to 4084\n",
    "\n",
    "America_dataframe_with_ingredients = np.zeros((America_dataframe_shape, np.shape(binary_assignment)[1]))\n",
    "for i in range(America_dataframe_shape):\n",
    "    America_dataframe_with_ingredients[i,:] = binary_assignment[int(np.array(America_dataframe.index)[i]),:]\n",
    "\n",
    "\n",
    "# Check ingredients that does not belong to any recipe (Let's call it X)\n",
    "# Turns out 2630 ingredients belong to none of the recipe in America (common_ingred_America_belong_none)\n",
    "Number_of_Receipe_America = np.shape(America_dataframe_with_ingredients)[0]\n",
    "Number_of_ingredients_America = np.shape(America_dataframe_with_ingredients)[1]\n",
    "\n",
    "common_ingred_America_belong_none = 0\n",
    "common_ingred_America_belong_all = 0\n",
    "List_for_America_X = []\n",
    "List_for_America_Y = []\n",
    "\n",
    "for i in range(Number_of_ingredients_America):\n",
    "    if np.sum(America_dataframe_with_ingredients[:,i]) == 0:\n",
    "        List_for_America_X.append(i)\n",
    "        common_ingred_America_belong_none += 1\n",
    "\n",
    "America_dataframe_with_ingredients_after_deleting_X = np.delete(America_dataframe_with_ingredients, List_for_America_X, axis=1)\n",
    "Number_of_ingredients_America_updated = np.shape(America_dataframe_with_ingredients_after_deleting_X)[1]\n",
    "\n",
    "# Check ingredients that belong to all receipe (common_ingred_America_belong_all)\n",
    "# Turns out to be 0 ingredients belong to all of the recipe in America\n",
    "for i in range(Number_of_ingredients_America_updated):\n",
    "    if np.sum(America_dataframe_with_ingredients_after_deleting_X[:,i]) == America_dataframe_with_ingredients_after_deleting_X.shape[0]:\n",
    "        List_for_America_Y.append(i)\n",
    "        common_ingred_America_belong_all += 1        \n",
    "\n",
    "America_dataframe_with_ingredients_final = np.delete(America_dataframe_with_ingredients_after_deleting_X, List_for_America_Y, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############Asia\n",
    "# We reduce the features from 6714 to 3528\n",
    "\n",
    "Asia_dataframe_with_ingredients = np.zeros((Asia_dataframe_shape, np.shape(binary_assignment)[1]))\n",
    "for i in range(Asia_dataframe_shape):\n",
    "    Asia_dataframe_with_ingredients[i,:] = binary_assignment[int(np.array(Asia_dataframe.index)[i]),:]\n",
    "\n",
    "\n",
    "# Check ingredients that does not belong to any recipe (Let's call it X)\n",
    "# Turns out 3186 ingredients belong to none of the recipe in Asia (common_ingred_Asia_belong_none)\n",
    "Number_of_Receipe_Asia = np.shape(Asia_dataframe_with_ingredients)[0]\n",
    "Number_of_ingredients_Asia = np.shape(Asia_dataframe_with_ingredients)[1]\n",
    "\n",
    "common_ingred_Asia_belong_none = 0\n",
    "common_ingred_Asia_belong_all = 0\n",
    "List_for_Asia_X = []\n",
    "List_for_Asia_Y = []\n",
    "\n",
    "for i in range(Number_of_ingredients_Asia):\n",
    "    if np.sum(Asia_dataframe_with_ingredients[:,i]) == 0:\n",
    "        List_for_Asia_X.append(i)\n",
    "        common_ingred_Asia_belong_none += 1\n",
    "\n",
    "Asia_dataframe_with_ingredients_after_deleting_X = np.delete(Asia_dataframe_with_ingredients, List_for_Asia_X, axis=1)\n",
    "Number_of_ingredients_Asia_updated = np.shape(Asia_dataframe_with_ingredients_after_deleting_X)[1]\n",
    "\n",
    "# Check ingredients that belong to all receipe (common_ingred_Asia_belong_all)\n",
    "# Turns out to be 0 ingredients belong to all of the recipe in Asia\n",
    "for i in range(Number_of_ingredients_Asia_updated):\n",
    "    if np.sum(Asia_dataframe_with_ingredients_after_deleting_X[:,i]) == Asia_dataframe_with_ingredients_after_deleting_X.shape[0]:\n",
    "        List_for_Asia_Y.append(i)\n",
    "        common_ingred_Asia_belong_all += 1        \n",
    "\n",
    "Asia_dataframe_with_ingredients_final = np.delete(Asia_dataframe_with_ingredients_after_deleting_X, List_for_Asia_Y, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############EU\n",
    "# We reduce the features from 6714 to 4195\n",
    "\n",
    "EU_dataframe_with_ingredients = np.zeros((EU_dataframe_shape, np.shape(binary_assignment)[1]))\n",
    "for i in range(EU_dataframe_shape):\n",
    "    EU_dataframe_with_ingredients[i,:] = binary_assignment[int(np.array(EU_dataframe.index)[i]),:]\n",
    "\n",
    "\n",
    "# Check ingredients that does not belong to any recipe (Let's call it X)\n",
    "# Turns out 2519 ingredients belong to none of the recipe in EU (common_ingred_EU_belong_none)\n",
    "Number_of_Receipe_EU = np.shape(EU_dataframe_with_ingredients)[0]\n",
    "Number_of_ingredients_EU = np.shape(EU_dataframe_with_ingredients)[1]\n",
    "\n",
    "common_ingred_EU_belong_none = 0\n",
    "common_ingred_EU_belong_all = 0\n",
    "List_for_EU_X = []\n",
    "List_for_EU_Y = []\n",
    "\n",
    "for i in range(Number_of_ingredients_EU):\n",
    "    if np.sum(EU_dataframe_with_ingredients[:,i]) == 0:\n",
    "        List_for_EU_X.append(i)\n",
    "        common_ingred_EU_belong_none += 1\n",
    "\n",
    "EU_dataframe_with_ingredients_after_deleting_X = np.delete(EU_dataframe_with_ingredients, List_for_EU_X, axis=1)\n",
    "Number_of_ingredients_EU_updated = np.shape(EU_dataframe_with_ingredients_after_deleting_X)[1]\n",
    "\n",
    "# Check ingredients that belong to all receipe (common_ingred_EU_belong_all)\n",
    "# Turns out to be 0 ingredients belong to all of the recipe in EU\n",
    "for i in range(Number_of_ingredients_EU_updated):\n",
    "    if np.sum(EU_dataframe_with_ingredients_after_deleting_X[:,i]) == EU_dataframe_with_ingredients_after_deleting_X.shape[0]:\n",
    "        List_for_EU_Y.append(i)\n",
    "        common_ingred_EU_belong_all += 1        \n",
    "\n",
    "EU_dataframe_with_ingredients_final = np.delete(EU_dataframe_with_ingredients_after_deleting_X, List_for_EU_Y, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############Africa\n",
    "# We reduce the features from 6714 to 974\n",
    "\n",
    "Other_dataframe_with_ingredients = np.zeros((Other_dataframe_shape, np.shape(binary_assignment)[1]))\n",
    "for i in range(Other_dataframe_shape):\n",
    "    Other_dataframe_with_ingredients[i,:] = binary_assignment[int(np.array(Other_dataframe.index)[i]),:]\n",
    "\n",
    "\n",
    "# Check ingredients that does not belong to any recipe (Let's call it X)\n",
    "# Turns out 5740 ingredients belong to none of the recipe in Other (common_ingred_Other_belong_none)\n",
    "Number_of_Receipe_Other = np.shape(Other_dataframe_with_ingredients)[0]\n",
    "Number_of_ingredients_Other = np.shape(Other_dataframe_with_ingredients)[1]\n",
    "\n",
    "common_ingred_Other_belong_none = 0\n",
    "common_ingred_Other_belong_all = 0\n",
    "List_for_Other_X = []\n",
    "List_for_Other_Y = []\n",
    "\n",
    "for i in range(Number_of_ingredients_Other):\n",
    "    if np.sum(Other_dataframe_with_ingredients[:,i]) == 0:\n",
    "        List_for_Other_X.append(i)\n",
    "        common_ingred_Other_belong_none += 1\n",
    "\n",
    "Other_dataframe_with_ingredients_after_deleting_X = np.delete(Other_dataframe_with_ingredients, List_for_Other_X, axis=1)\n",
    "Number_of_ingredients_Other_updated = np.shape(Other_dataframe_with_ingredients_after_deleting_X)[1]\n",
    "\n",
    "# Check ingredients that belong to all receipe (common_ingred_Other_belong_all)\n",
    "# Turns out to be 0 ingredients belong to all of the recipe in Other\n",
    "for i in range(Number_of_ingredients_Other_updated):\n",
    "    if np.sum(Other_dataframe_with_ingredients_after_deleting_X[:,i]) == Other_dataframe_with_ingredients_after_deleting_X.shape[0]:\n",
    "        List_for_Other_Y.append(i)\n",
    "        common_ingred_Other_belong_all += 1        \n",
    "\n",
    "Other_dataframe_with_ingredients_final = np.delete(Other_dataframe_with_ingredients_after_deleting_X, List_for_Other_Y, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Correspondence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mca\n",
    "from mca import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n",
    "pd.set_option('display.precision', 5)\n",
    "pd.set_option('display.max_columns', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_assignment_dataframe = pd.DataFrame(binary_assignment)\n",
    "np.shape(binary_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Receipe = np.shape(binary_assignment)[0]\n",
    "Number_of_ingredients = np.shape(binary_assignment)[1] \n",
    "revised_binary_assignment = np.zeros((Number_of_Receipe,Number_of_ingredients *2))\n",
    "\n",
    "for i in range(Number_of_Receipe):\n",
    "    for j in range(0, Number_of_ingredients, 2):\n",
    "        if binary_assignment[i][j] == 1:\n",
    "            revised_binary_assignment[i][j] = 1\n",
    "            revised_binary_assignment[i][j+1] = 0\n",
    "        else:\n",
    "            revised_binary_assignment[i][j] = 0\n",
    "            revised_binary_assignment[i][j+1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_binary_assignment_dataframe = pd.DataFrame(revised_binary_assignment)\n",
    "revised_binary_assignment_dataframe\n",
    "ncols = np.shape(revised_binary_assignment_dataframe)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(revised_binary_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_ben = MCA(revised_binary_assignment_dataframe, ncols=ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca_ind = MCA(revised_binary_assignment_dataframe, ncols=ncols, benzecri=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mca.MCA.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Iλ': pd.Series(mca_ind.L),\n",
    "        'τI': mca_ind.expl_var(greenacre=False, N=4),\n",
    "        'Zλ': pd.Series(mca_ben.L),\n",
    "        'τZ': mca_ben.expl_var(greenacre=False, N=4),\n",
    "        'cλ': pd.Series(mca_ben.L),\n",
    "        'τc': mca_ind.expl_var(greenacre=True, N=4)}\n",
    "\n",
    "# 'Indicator Matrix', 'Benzecri Correction', 'Greenacre Correction'\n",
    "columns = ['Iλ', 'τI', 'Zλ', 'τZ', 'cλ', 'τc']\n",
    "table2 = pd.DataFrame(data=data, columns=columns).fillna(0)\n",
    "table2.index += 1\n",
    "table2.loc['Σ'] = table2.sum()\n",
    "table2.index.name = 'Factor'\n",
    "\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ShuffleSplit(n_splits=10, test_size=0.1, train_size=0.9)\n",
    "clf = LogisticRegression(C=1,multi_class='multinomial',penalty='l1', solver='saga', tol=0.05)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=clf.predict(X_test)\n",
    "y_train_pred=clf.predict(X_train)\n",
    "accuracy_test = np.sum(y_test_pred == y_test) / len(y_test)\n",
    "accuracy_train = np.sum(y_train_pred == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\tAccuracy of test', accuracy_test)\n",
    "print('\\tAccuracy of training',accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ovr = LogisticRegression(C=1,multi_class='ovr',penalty='l1', solver='saga', tol=0.05)\n",
    "clf_ovr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred1=clf_ovr.predict(X_test)\n",
    "y_train_pred1=clf_ovr.predict(X_train)\n",
    "accuracy_test1 = np.sum(y_test_pred1 == y_test) / len(y_test)\n",
    "accuracy_train1 = np.sum(y_train_pred1 == y_train) / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\tAccuracy of test', accuracy_test1)\n",
    "print('\\tAccuracy of training',accuracy_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classif = OneVsRestClassifier(SVM(kernel='linear'))\n",
    "svm_classif.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "A = pickle.load(open('SVM_cont','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=5, random_state=None, test_size=0.1, train_size=0.9),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': array([0.01, 0.12, 0.23, 0.34, 0.45, 0.56, 0.67, 0.78, 0.89, 1.  ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
